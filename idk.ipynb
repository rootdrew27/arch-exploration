{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import fiftyone\n",
    "import math\n",
    "import transformers\n",
    "from utils.preprocessing import bbs_corners_to_centers, normalize_bbs \n",
    "\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, channels_out=64, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "#         super().__init__(ConvBlock, self)\n",
    "#         self.conv = nn.LazyConv2d(channels_out, kernel_size, stride, padding, bias=False) \n",
    "#         self.pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn = nn.BatchNorm2d(channels_out)\n",
    "#         self.act = nn.SiLU()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.act(self.bn(self.pool(self.conv(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, num_channels, use1x1=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        if use1x1:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, strides=strides)\n",
    "            \n",
    "        else:\n",
    "            self.conv3 = None\n",
    "            \n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.SiLU(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "        y += x\n",
    "        return F.SiLU(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackBone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(BackBone, self)\n",
    "        layer1 = nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3), # Downsample 1/2\n",
    "            nn.LazyBatchNorm2d(), nn.SiLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # Downsample 1/2\n",
    "        )\n",
    "        layer2 = Residual(64, True, 1)\n",
    "        layer3 = Residual(32, True, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, q, k, v, temp=None):\n",
    "        \n",
    "        if temp == None:\n",
    "          temp = k.size(-1)  # increasing temp will \"flatten\" the attention distribution \n",
    "        \n",
    "        print(q.shape)\n",
    "        \n",
    "        att = q @ k.transpose(-2, -1) / math.sqrt(temp) # B, nh, T, T \n",
    "        att = F.softmax(att, dim=-1) \n",
    "        y = att @ v # B, nh, T, hs\n",
    "        \n",
    "        return y.transpose(1,2).contiguous()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, o_c, k_s, s=1, p=0, downsample=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in1 = nn.LazyInstanceNorm2d()\n",
    "        # self.in2 = nn.LazyInstanceNorm2d()\n",
    "        \n",
    "        self.conv1 = nn.LazyConv2d(o_c, k_s, s, p)\n",
    "        self.pool = nn.MaxPool2d(2, 2) if downsample else nn.MaxPool2d(3, 1, 1)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.pool(self.conv1(self.in1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, emb_d, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert emb_d % num_heads == 0\n",
    "        \n",
    "        self.emb_d = emb_d\n",
    "        self.num_heads = num_heads\n",
    "        self.c_attn = nn.Linear(emb_d, 3 * emb_d, bias=False)         \n",
    "        self.attention = SelfAttention()\n",
    "        self.conv = ConvBlock(emb_d, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "                    \n",
    "        B, C, H, W = x.size()\n",
    "        T = W * H\n",
    "        x = x.view(B, C, T).transpose(1,2) # (B, T, C)\n",
    "        \n",
    "        q, k, v = self.c_attn(x).split(self.emb_d, dim=2)\n",
    "        \n",
    "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        y = self.attention(q, k, v) # B, nh, hs, T\n",
    "        \n",
    "        y = y.view(B,T,C).transpose(1,2).view(B,C,H,W)\n",
    "        out = self.conv(y)\n",
    "                \n",
    "        return out\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  2,  3],\n",
       "          [ 4,  5,  6]],\n",
       "\n",
       "         [[ 7,  8,  9],\n",
       "          [10, 11, 12]],\n",
       "\n",
       "         [[13, 14, 15],\n",
       "          [16, 17, 18]],\n",
       "\n",
       "         [[19, 20, 21],\n",
       "          [22, 23, 24]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[[1,2,3], [4,5,6]], [[7,8,9], [10,11,12]], [[13,14,15], [16,17,18]],[[19,20,21], [22,23,24]]]])\n",
    "                # channel 1            # channel 2\n",
    "                \n",
    "nh = 2\n",
    "                \n",
    "B, C, H, W = x.size()\n",
    "\n",
    "T = W * H\n",
    "\n",
    "# x.view(B,C,T).transpose(1,2).view(B, T, nh, C // nh).transpose(1,2).contiguous().transpose(-2,-1).reshape(B,C,T).view(B,C,H,W)\n",
    "x.view(B,C,T).transpose(1,2).view(B, T, nh, C // nh).transpose(1,2).transpose(1,2).contiguous().view(B,T,C).transpose(1,2).view(B,C,H,W)\n",
    "# x.view(B,C,T).transpose(1,2).view(B, T, nh, C // nh).transpose(1,2).view(B, C, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,  num_classes:int):\n",
    "        super(Net, self).__init__()\n",
    "                \n",
    "        self.conv1 = ConvBlock(512, 7, 1, 3)\n",
    "        # self.mha1 = nn.MultiheadAttention(512, 8) \n",
    "        self.conv2 = ConvBlock(512, 5, 1, 2, downsample=True)\n",
    "        # self.mha2 = nn.MultiheadAttention(512, 8)\n",
    "        self.conv3 = ConvBlock(512, 3, 1, 1, downsample=True)\n",
    "        \n",
    "        self.bb_pred = nn.Sequential(\n",
    "            ConvBlock(256, 3, 1, 1, downsample=True), ConvBlock(4, 1, 1, 0), ConvBlock(4, 3, 1, 0), nn.Flatten()\n",
    "        ) # (B, 4, H/8, W/8)\n",
    "        \n",
    "        self.class_pred = nn.Sequential(\n",
    "            ConvBlock(256, 3, 1, 1, downsample=True), ConvBlock(num_classes, 1, 1, 0), ConvBlock(num_classes, 1, 1), nn.Softmax(1) \n",
    "        ) # (B, num_classes, H/8, W/8)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = x.view(x.shape[0], 512, -1).transpose(-2, -1)\n",
    "        x, _ = self.mha1(x, x, x)\n",
    "        x = x.transpose(-2,-1).view(x.shape[0], 512, 128, 128)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.shape[0], 512, -1).transpose(-2, -1)\n",
    "        x, _ = self.mha2(x, x, x)\n",
    "        x = x.transpose(-2,-1).view(x.shape[0], 512, 64, 64)\n",
    "        x = self.conv3(x)\n",
    "                \n",
    "        return self.bb_pred(x), self.class_pred(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0,0,0), (1,1,1))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.VOCDetection('data\\PASCALVOC', image_set='train', transform=transform, download=False)\n",
    "\n",
    "val_dataset = datasets.VOCDetection('data\\PASCALVOC', image_set='val', transform=transform, download=False)\n",
    "\n",
    "# trainval_dataset = datasets.VOCDetection('data\\PASCALVOC', image_set='trainval', transform=transform, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2_idx = {'pottedplant': 1, 'person': 2,\n",
    "               'motorbike': 3, 'train': 4,\n",
    "               'dog': 5, 'diningtable': 6,\n",
    "               'horse': 7, 'bus': 8,\n",
    "               'aeroplane': 9, 'sofa': 10,\n",
    "               'sheep': 11, 'tvmonitor': 12,\n",
    "               'bird': 13, 'bottle': 14,\n",
    "               'chair': 15, 'cat': 16,\n",
    "               'bicycle': 17, 'cow': 18,\n",
    "               'boat': 19, 'car': 20, 'bg': 0}\n",
    "\n",
    "idx_2_label = {1: 'pottedplant', 2: 'person',\n",
    "               3: 'motorbike', 4: 'train',\n",
    "               5: 'dog', 6: 'diningtable',\n",
    "               7: 'horse', 8: 'bus',\n",
    "               9: 'aeroplane', 10: 'sofa',\n",
    "               11: 'sheep', 12: 'tvmonitor',\n",
    "               13: 'bird', 14: 'bottle',\n",
    "               15: 'chair', 16: 'cat',\n",
    "               17: 'bicycle', 18: 'cow',\n",
    "               19: 'boat', 20: 'car', 0: 'bg'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVOCDetection(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations:pd.DataFrame, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = modify_dataset(train_dataset, label_2_idx)\n",
    "val_df = modify_dataset(val_dataset, label_2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustomVOCDetection(train_df, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert this into a \"target_transform\" function that'll be used by the Dataset object\n",
    "\n",
    "def modify_dataset(dataset:torch.utils.data.Dataset, label_2_idx_map:dict) -> pd.DataFrame:\n",
    "\n",
    "    try:                   \n",
    "\n",
    "        new_labels = []\n",
    "        \n",
    "        for i in range(len(dataset)):\n",
    "\n",
    "            annotation = dataset[i][1]['annotation']\n",
    "\n",
    "            imgw = int(annotation['size']['width'])  \n",
    "            imgh = int(annotation['size']['height'])         \n",
    "\n",
    "            labels = annotation['object']\n",
    "\n",
    "            bbs = []\n",
    "            classes = []\n",
    "\n",
    "            assert isinstance(labels, (list, dict)) \n",
    "\n",
    "            if isinstance(labels, list):\n",
    "                \n",
    "                for obj in labels:\n",
    "                    # TODO replace innards with function call(s)\n",
    "                    class_num = label_2_idx_map[obj['name']] if type(obj['name']) == str else label_2_idx_map[obj['name'][0]]\n",
    "\n",
    "                    bb = obj['bndbox']\n",
    "                    \n",
    "                    assert type(bb) == dict\n",
    "\n",
    "                    x1 = int(bb['xmin'][0]) if type(bb['xmin']) == list else int(bb['xmin'])\n",
    "                    x2 = int(bb['xmax'][0]) if type(bb['xmax']) == list else int(bb['xmax'])\n",
    "                    y1 = int(bb['ymin'][0]) if type(bb['ymin']) == list else int(bb['ymin'])\n",
    "                    y2 = int(bb['ymax'][0]) if type(bb['ymax']) == list else int(bb['ymax'])\n",
    "\n",
    "                    bb = [x1, y1, x2, y2]\n",
    "                    bbs.append(bb)\n",
    "                    classes.append(class_num)\n",
    "\n",
    "            elif isinstance(labels, dict):\n",
    "                # TODO replace innards with function call(s)\n",
    "                obj = labels\n",
    "\n",
    "                class_num = label_2_idx_map[obj['name']] if type(obj['name']) == str else label_2_idx_map[obj['name'][0]]\n",
    "\n",
    "                assert type(bb) == dict\n",
    "\n",
    "                bb = obj['bndbox']\n",
    "                x1 = int(bb['xmin'][0]) if type(bb['xmin']) == list else int(bb['xmin'])\n",
    "                x2 = int(bb['xmax'][0]) if type(bb['xmax']) == list else int(bb['xmax'])\n",
    "                y1 = int(bb['ymin'][0]) if type(bb['ymin']) == list else int(bb['ymin'])\n",
    "                y2 = int(bb['ymax'][0]) if type(bb['ymax']) == list else int(bb['ymax'])\n",
    "\n",
    "                bb = [x1, y1, x2, y2]\n",
    "                bbs.append(bb)\n",
    "\n",
    "            bbs = torch.tensor(bbs)\n",
    "            bbs = bbs_corners_to_centers(bbs)\n",
    "            bbs = normalize_bbs(bbs, (imgw, imgh))\n",
    "            \n",
    "            new_label = (bbs, torch.tensor(classes))            \n",
    "            new_labels.append(new_label)\n",
    "                            \n",
    "        return pd.DataFrame(new_labels)\n",
    "    \n",
    "    except AssertionError:\n",
    "        print(f'Data entry {i} is not formatted as expected.\\nNote that none of the dataset has been updated')\n",
    "        return None\n",
    "    \n",
    "    except Exception as ex:\n",
    "        logging.error(f'A general exception was caught: {ex}', exc_info=True)\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 5717 instances\n",
      "Validation set has 5823 instances\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "# trainval_loader = torch.utils.data.DataLoader(trainval_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# print('Trainval set has {} instances'.format(len(trainval_loader)))\n",
    "\n",
    "print('Training set has {} instances'.format(len(train_loader)))\n",
    "print('Validation set has {} instances'.format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'folder': 'VOC2012', 'filename': '2008_000008.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '442', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'horse', 'pose': 'Left', 'truncated': '0', 'occluded': '1', 'bndbox': {'xmin': '53', 'ymin': '87', 'xmax': '471', 'ymax': '420'}, 'difficult': '0'}, {'name': 'person', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '158', 'ymin': '44', 'xmax': '289', 'ymax': '167'}, 'difficult': '0'}]}\n",
      "{'folder': 'VOC2012', 'filename': '2008_000015.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '327', 'depth': '3'}, 'segmented': '1', 'object': [{'name': 'bottle', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '1', 'bndbox': {'xmin': '270', 'ymin': '1', 'xmax': '378', 'ymax': '176'}, 'difficult': '0'}, {'name': 'bottle', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '1', 'bndbox': {'xmin': '57', 'ymin': '1', 'xmax': '164', 'ymax': '150'}, 'difficult': '0'}]}\n",
      "{'folder': 'VOC2012', 'filename': '2008_000019.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '480', 'height': '272', 'depth': '3'}, 'segmented': '1', 'object': [{'name': 'dog', 'pose': 'Unspecified', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '139', 'ymin': '2', 'xmax': '372', 'ymax': '197'}, 'difficult': '0'}, {'name': 'dog', 'pose': 'Unspecified', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '165', 'ymin': '66', 'xmax': '318', 'ymax': '236'}, 'difficult': '0'}, {'name': 'dog', 'pose': 'Right', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '361', 'ymin': '1', 'xmax': '480', 'ymax': '112'}, 'difficult': '0'}]}\n",
      "{'folder': 'VOC2012', 'filename': '2008_000023.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '333', 'height': '500', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'tvmonitor', 'pose': 'Frontal', 'truncated': '1', 'occluded': '1', 'bndbox': {'xmin': '6', 'ymin': '1', 'xmax': '314', 'ymax': '262'}, 'difficult': '0'}, {'name': 'bottle', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '1', 'bndbox': {'xmin': '40', 'ymin': '97', 'xmax': '121', 'ymax': '411'}, 'difficult': '0'}, {'name': 'person', 'pose': 'Frontal', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '137', 'ymin': '36', 'xmax': '169', 'ymax': '109'}, 'difficult': '0'}, {'name': 'person', 'pose': 'Frontal', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '180', 'ymin': '36', 'xmax': '216', 'ymax': '104'}, 'difficult': '0'}, {'name': 'person', 'pose': 'Frontal', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '96', 'ymin': '39', 'xmax': '123', 'ymax': '103'}, 'difficult': '0'}]}\n",
      "{'folder': 'VOC2012', 'filename': '2008_000028.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '375', 'depth': '3'}, 'segmented': '1', 'object': [{'name': 'car', 'pose': 'Unspecified', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '158', 'ymin': '199', 'xmax': '231', 'ymax': '240'}, 'difficult': '0'}, {'name': 'car', 'pose': 'Frontal', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '47', 'ymin': '202', 'xmax': '94', 'ymax': '238'}, 'difficult': '0'}]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    \n",
    "    if i == 5: break\n",
    "        \n",
    "    print(train_dataset[i][1]['annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[272, 111, 318, 155], [212, 131, 309, 216], [1, 1, 437, 333], [2, 30, 500, 333]]\n",
      "\n",
      "\n",
      "tensor([[272, 111, 318, 155],\n",
      "        [212, 131, 309, 216],\n",
      "        [  1,   1, 437, 333],\n",
      "        [  2,  30, 500, 333]])\n",
      "\n",
      "\n",
      "tensor([[295.0000, 133.0000,  46.0000,  44.0000],\n",
      "        [260.5000, 173.5000,  97.0000,  85.0000],\n",
      "        [219.0000, 167.0000, 436.0000, 332.0000],\n",
      "        [251.0000, 181.5000, 498.0000, 303.0000]])\n",
      "\n",
      "\n",
      "img w: 500, img h: 333\n",
      "tensor([[0.5900, 0.3994, 0.1381, 0.1321],\n",
      "        [0.5210, 0.5210, 0.2913, 0.2553],\n",
      "        [0.4380, 0.5015, 1.3093, 0.9970],\n",
      "        [0.5020, 0.5450, 1.4955, 0.9099]])\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import bbs_corners_to_centers, normalize_bbs\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    if i == 1: break\n",
    "    inputs, labels = data\n",
    "    \n",
    "    imgw = int(labels['annotation']['size']['width'][0])\n",
    "    imgh = int(labels['annotation']['size']['height'][0])\n",
    "    \n",
    "    labels = labels['annotation']['object']\n",
    "        \n",
    "    if type(labels) == list:\n",
    "    \n",
    "        bbs = []\n",
    "\n",
    "        for obj in labels:\n",
    "\n",
    "            name = obj['name']\n",
    "\n",
    "            bb = obj['bndbox']\n",
    "            x1 = int(bb['xmin'][0]) if type(bb['xmin']) == list else bb['xmin']        \n",
    "            x2 = int(bb['xmax'][0]) if type(bb['xmax']) == list else bb['xmax']\n",
    "            y1 = int(bb['ymin'][0]) if type(bb['ymin']) == list else bb['ymin']\n",
    "            y2 = int(bb['ymax'][0]) if type(bb['ymax']) == list else bb['ymax']\n",
    "\n",
    "            bb = [x1, y1, x2, y2]\n",
    "            bbs.append(bb)\n",
    "\n",
    "        bb_lbl = torch.tensor(bbs)\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(i)\n",
    "    print(bbs)\n",
    "    print('\\n')\n",
    "    print(bb_lbl)\n",
    "    print('\\n')\n",
    "    print(bbs_corners_to_centers(bb_lbl))\n",
    "    print('\\n')\n",
    "    print(f'img w: {imgw}, img h: {imgh}')\n",
    "    print(normalize_bbs(bbs_corners_to_centers(bb_lbl), imgsz=(imgw, imgh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tvmonitor',\n",
       "  'pose': 'Frontal',\n",
       "  'truncated': '1',\n",
       "  'occluded': '1',\n",
       "  'bndbox': {'xmin': '6', 'ymin': '1', 'xmax': '314', 'ymax': '262'},\n",
       "  'difficult': '0'},\n",
       " {'name': 'bottle',\n",
       "  'pose': 'Unspecified',\n",
       "  'truncated': '1',\n",
       "  'occluded': '1',\n",
       "  'bndbox': {'xmin': '40', 'ymin': '97', 'xmax': '121', 'ymax': '411'},\n",
       "  'difficult': '0'},\n",
       " {'name': 'person',\n",
       "  'pose': 'Frontal',\n",
       "  'truncated': '1',\n",
       "  'occluded': '0',\n",
       "  'bndbox': {'xmin': '137', 'ymin': '36', 'xmax': '169', 'ymax': '109'},\n",
       "  'difficult': '0'},\n",
       " {'name': 'person',\n",
       "  'pose': 'Frontal',\n",
       "  'truncated': '1',\n",
       "  'occluded': '0',\n",
       "  'bndbox': {'xmin': '180', 'ymin': '36', 'xmax': '216', 'ymax': '104'},\n",
       "  'difficult': '0'},\n",
       " {'name': 'person',\n",
       "  'pose': 'Frontal',\n",
       "  'truncated': '1',\n",
       "  'occluded': '0',\n",
       "  'bndbox': {'xmin': '96', 'ymin': '39', 'xmax': '123', 'ymax': '103'},\n",
       "  'difficult': '0'}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3][1]['annotation']['object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.LazyLinear(10, bias=False)\n",
    "X = torch.randn(1,1,5)\n",
    "lin(X)\n",
    "[p.numel() for p in lin.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_classes=2).to('cuda')\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11685810"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5570, 0.8553, 0.5165, 0.5202],\n",
       "         [0.5570, 0.8553, 0.5164, 0.5202],\n",
       "         [0.5570, 0.8553, 0.5164, 0.5202],\n",
       "         [0.5570, 0.8553, 0.5164, 0.5202],\n",
       "         [0.5570, 0.8553, 0.5165, 0.5202]], device='cuda:0',\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[[0.4967, 0.4967, 0.4967,  ..., 0.4986, 0.5604, 0.6707],\n",
       "           [0.4967, 0.3777, 0.3777,  ..., 0.3954, 0.6477, 0.7461],\n",
       "           [0.5600, 0.4381, 0.4133,  ..., 0.3967, 0.6511, 0.7461],\n",
       "           ...,\n",
       "           [0.1526, 0.1526, 0.2365,  ..., 0.3529, 0.4746, 0.4746],\n",
       "           [0.7494, 0.7494, 0.6167,  ..., 0.5186, 0.6356, 0.6356],\n",
       "           [0.7677, 0.7677, 0.6167,  ..., 0.5186, 0.6356, 0.6356]],\n",
       " \n",
       "          [[0.5033, 0.5033, 0.5033,  ..., 0.5014, 0.4396, 0.3293],\n",
       "           [0.5033, 0.6223, 0.6223,  ..., 0.6046, 0.3523, 0.2539],\n",
       "           [0.4400, 0.5619, 0.5867,  ..., 0.6033, 0.3489, 0.2539],\n",
       "           ...,\n",
       "           [0.8474, 0.8474, 0.7635,  ..., 0.6471, 0.5254, 0.5254],\n",
       "           [0.2506, 0.2506, 0.3833,  ..., 0.4814, 0.3644, 0.3644],\n",
       "           [0.2323, 0.2323, 0.3833,  ..., 0.4814, 0.3644, 0.3644]]],\n",
       " \n",
       " \n",
       "         [[[0.4968, 0.4968, 0.4968,  ..., 0.4986, 0.5604, 0.6707],\n",
       "           [0.4968, 0.3777, 0.3777,  ..., 0.3954, 0.6477, 0.7460],\n",
       "           [0.5601, 0.4381, 0.4133,  ..., 0.3966, 0.6511, 0.7460],\n",
       "           ...,\n",
       "           [0.1526, 0.1526, 0.2366,  ..., 0.3528, 0.4745, 0.4745],\n",
       "           [0.7494, 0.7494, 0.6168,  ..., 0.5184, 0.6355, 0.6355],\n",
       "           [0.7675, 0.7675, 0.6168,  ..., 0.5184, 0.6355, 0.6355]],\n",
       " \n",
       "          [[0.5032, 0.5032, 0.5032,  ..., 0.5014, 0.4396, 0.3293],\n",
       "           [0.5032, 0.6223, 0.6223,  ..., 0.6046, 0.3523, 0.2540],\n",
       "           [0.4399, 0.5619, 0.5867,  ..., 0.6034, 0.3489, 0.2540],\n",
       "           ...,\n",
       "           [0.8474, 0.8474, 0.7634,  ..., 0.6472, 0.5255, 0.5255],\n",
       "           [0.2506, 0.2506, 0.3832,  ..., 0.4816, 0.3645, 0.3645],\n",
       "           [0.2325, 0.2325, 0.3832,  ..., 0.4816, 0.3645, 0.3645]]],\n",
       " \n",
       " \n",
       "         [[[0.4966, 0.4966, 0.4966,  ..., 0.4986, 0.5604, 0.6707],\n",
       "           [0.4966, 0.3776, 0.3776,  ..., 0.3954, 0.6477, 0.7460],\n",
       "           [0.5600, 0.4382, 0.4133,  ..., 0.3967, 0.6511, 0.7460],\n",
       "           ...,\n",
       "           [0.1526, 0.1526, 0.2366,  ..., 0.3527, 0.4744, 0.4744],\n",
       "           [0.7494, 0.7494, 0.6168,  ..., 0.5185, 0.6354, 0.6354],\n",
       "           [0.7676, 0.7676, 0.6168,  ..., 0.5185, 0.6354, 0.6354]],\n",
       " \n",
       "          [[0.5034, 0.5034, 0.5034,  ..., 0.5014, 0.4396, 0.3293],\n",
       "           [0.5034, 0.6224, 0.6224,  ..., 0.6046, 0.3523, 0.2540],\n",
       "           [0.4400, 0.5618, 0.5867,  ..., 0.6033, 0.3489, 0.2540],\n",
       "           ...,\n",
       "           [0.8474, 0.8474, 0.7634,  ..., 0.6473, 0.5256, 0.5256],\n",
       "           [0.2506, 0.2506, 0.3832,  ..., 0.4815, 0.3646, 0.3646],\n",
       "           [0.2324, 0.2324, 0.3832,  ..., 0.4815, 0.3646, 0.3646]]],\n",
       " \n",
       " \n",
       "         [[[0.4967, 0.4967, 0.4967,  ..., 0.4985, 0.5604, 0.6708],\n",
       "           [0.4967, 0.3776, 0.3776,  ..., 0.3952, 0.6476, 0.7460],\n",
       "           [0.5599, 0.4380, 0.4132,  ..., 0.3965, 0.6510, 0.7460],\n",
       "           ...,\n",
       "           [0.1527, 0.1527, 0.2366,  ..., 0.3528, 0.4745, 0.4745],\n",
       "           [0.7495, 0.7495, 0.6169,  ..., 0.5185, 0.6355, 0.6355],\n",
       "           [0.7676, 0.7676, 0.6169,  ..., 0.5185, 0.6355, 0.6355]],\n",
       " \n",
       "          [[0.5034, 0.5034, 0.5034,  ..., 0.5015, 0.4396, 0.3292],\n",
       "           [0.5034, 0.6224, 0.6224,  ..., 0.6048, 0.3524, 0.2540],\n",
       "           [0.4401, 0.5620, 0.5868,  ..., 0.6035, 0.3490, 0.2540],\n",
       "           ...,\n",
       "           [0.8473, 0.8473, 0.7634,  ..., 0.6472, 0.5255, 0.5255],\n",
       "           [0.2505, 0.2505, 0.3831,  ..., 0.4815, 0.3645, 0.3645],\n",
       "           [0.2324, 0.2324, 0.3831,  ..., 0.4815, 0.3645, 0.3645]]],\n",
       " \n",
       " \n",
       "         [[[0.4968, 0.4968, 0.4968,  ..., 0.4985, 0.5605, 0.6708],\n",
       "           [0.4968, 0.3777, 0.3777,  ..., 0.3953, 0.6477, 0.7460],\n",
       "           [0.5601, 0.4381, 0.4132,  ..., 0.3965, 0.6511, 0.7460],\n",
       "           ...,\n",
       "           [0.1526, 0.1526, 0.2366,  ..., 0.3528, 0.4744, 0.4744],\n",
       "           [0.7494, 0.7494, 0.6168,  ..., 0.5185, 0.6354, 0.6354],\n",
       "           [0.7676, 0.7676, 0.6168,  ..., 0.5185, 0.6354, 0.6354]],\n",
       " \n",
       "          [[0.5032, 0.5032, 0.5032,  ..., 0.5015, 0.4395, 0.3292],\n",
       "           [0.5032, 0.6223, 0.6223,  ..., 0.6047, 0.3523, 0.2540],\n",
       "           [0.4399, 0.5619, 0.5868,  ..., 0.6035, 0.3489, 0.2540],\n",
       "           ...,\n",
       "           [0.8474, 0.8474, 0.7634,  ..., 0.6472, 0.5256, 0.5256],\n",
       "           [0.2506, 0.2506, 0.3832,  ..., 0.4815, 0.3646, 0.3646],\n",
       "           [0.2324, 0.2324, 0.3832,  ..., 0.4815, 0.3646, 0.3646]]]],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(5, 3, 128, 128).to('cuda')\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = nn.Sequential(ConvBlock(512, 7, 1, 3, downsample=True), ConvBlock(512, 3, 1, 1, downsample=True), ConvBlock(4, 3, 1, 1), nn.FractionalMaxPool2d((1,1), 1), nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(5,3,16,16)\n",
    "Y = seq(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7605, 0.1296, 1.0942, 0.2514],\n",
       "        [0.5315, 1.1441, 0.4716, 0.3814],\n",
       "        [0.3145, 0.2099, 0.6240, 0.7039],\n",
       "        [0.0996, 0.7513, 0.4232, 1.0020],\n",
       "        [0.2571, 0.2531, 0.5619, 0.6083]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9524, 0.8184],\n",
       "        [0.0786, 0.1661],\n",
       "        [0.5139, 0.7119],\n",
       "        [0.3975, 0.8346],\n",
       "        [0.1803, 0.6426]])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_labels = torch.empty(5, 4).uniform_(0, to=1)\n",
    "bb_labels[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9524, 0.0786, 0.5139, 0.3975, 0.1803],\n",
       "        [0.8184, 0.1661, 0.7119, 0.8346, 0.6426]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_labels[:,:2].transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9768, 0.1982, 0.8496, 0.9961, 0.7670],\n",
       "        [0.5050, 0.0417, 0.2725, 0.2108, 0.0956],\n",
       "        [0.6510, 0.1230, 0.5413, 0.6182, 0.4661],\n",
       "        [1.4457, 0.2095, 1.0274, 1.0545, 0.7199],\n",
       "        [0.5524, 0.1020, 0.4527, 0.5126, 0.3835]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:, :2] @ bb_labels[:,:2].transpose(0,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 4\n",
      "\n",
      "\n",
      "Idx: 1\n",
      "\n",
      "\n",
      "Idx: 4\n",
      "\n",
      "\n",
      "Idx: 0\n",
      "\n",
      "\n",
      "Idx: 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for y in Y:\n",
    "    \n",
    "    # print(f'y:\\n{y[:2]}')\n",
    "    # print('\\n')\n",
    "    # print(f'bb_labels:\\n{bb_labels[:,:2]}')\n",
    "    # print('\\n')\n",
    "\n",
    "print(f'Idx: {torch.argmin(torch.abs((y[:2] - bb_labels[:,:2])).sum(-1))}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_targets(bb_preds, labels):\n",
    "    \"\"\"Calculate the nearest ground-truth bounding box for each prediciton and use that bb as the target\n",
    "\n",
    "    Args:\n",
    "        bb_preds : Predicted boudning boxes     \n",
    "        labels : Ground-truth labels\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    F.l(bb_preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 5.])\n",
      "tensor(0)\n",
      "tensor([1., 2., 4., 5.])\n",
      "\n",
      "\n",
      "tensor([1., 3., 4., 5.])\n",
      "tensor(0)\n",
      "tensor([1., 2., 4., 5.])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rooty\\AppData\\Local\\Temp\\ipykernel_13700\\2407261444.py:6: UserWarning: Using a target size (torch.Size([2, 2])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  minidx = torch.argmin(F.l1_loss(x[2:], Y[:,2:], reduction='none').sum(-1))\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3,5], [1,3,4,5]], dtype=torch.float)\n",
    "Y = torch.tensor([[1,2,4,5], [1,1,4,6]], dtype=torch.float)\n",
    "\n",
    "for x in X:\n",
    "    print(x)\n",
    "    minidx = torch.argmin(F.l1_loss(x[2:], Y[:,2:], reduction='none').sum(-1))\n",
    "    print(minidx)\n",
    "    print(Y[minidx])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, train, val, epochs=10, pp=100):\n",
    "    \n",
    "    model.train()\n",
    "    # c_loss_fn = nn.CrossEntropyLoss()\n",
    "    # bb_loss_fn = nn.L1Loss()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        inputs, labels = data\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        bb_pred, class_pred = model(inputs)\n",
    "                \n",
    "        targets = calc_targets(bb_preds, labels)\n",
    "                \n",
    "        bb_loss = F.l1_loss()\n",
    "        class_loss = F.cross_entropy()    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "loss = F.binary_cross_entropy_with_logits(input, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)\n",
    "# output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7048, 0.0671, 0.1291, 0.0421, 0.0569],\n",
       "        [0.0366, 0.4363, 0.0321, 0.4402, 0.0547],\n",
       "        [0.0550, 0.1117, 0.1777, 0.1477, 0.5079]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4872, -1.8913,  0.9910,  0.2958, -0.0855],\n",
      "        [ 0.8182,  2.1667, -1.1634, -0.6866, -0.5723],\n",
      "        [-0.2229, -0.3822,  0.8446,  0.4831, -0.6239]], requires_grad=True)\n",
      "tensor([2.2302, 3.0668, 1.9288], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0967, -0.6232,  0.1727], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.exp(output * -1) * (torch.exp(input)).sum(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Arch Exploration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
